OPENVOICE_FILES = {
    "google-bert/bert-base-multilingual-uncased": {
        "files": [
            "config.json",
            "model.safetensors",
            "tokenizer.json",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
    "google-bert/bert-base-uncased": {
        "files": [
            "config.json",
            "model.safetensors",
            "tokenizer.json",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
    "dbmdz/bert-base-french-europeana-cased": {
        "files": [
            "config.json",
            "pytorch_model.bin",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
    "dccuchile/bert-base-spanish-wwm-uncased": {
        "files": [
            "config.json",
            "pytorch_model.bin",
            "tokenizer.json",
            "special_tokens_map.json",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
    "kykim/bert-kor-base": {
        "files": [
            "config.json",
            "pytorch_model.bin",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
    "myshell-ai/MeloTTS-English": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "myshell-ai/MeloTTS-English-v3": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "myshell-ai/MeloTTS-French": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "myshell-ai/MeloTTS-Japanese": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "myshell-ai/MeloTTS-Spanish": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "myshell-ai/MeloTTS-Chinese": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "myshell-ai/MeloTTS-Korean": {
        "files": [
            "checkpoint.pth",
            "config.json",
        ]
    },
    "tohoku-nlp/bert-base-japanese-v3": {
        "files": [
            "config.json",
            "pytorch_model.bin",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
    "hfl/chinese-roberta-wwm-ext-large": {
        "files": [
            "added_tokens.json",
            "config.json",
            "pytorch_model.bin",
            "special_tokens_map.json",
            "tokenizer.json",
            "tokenizer_config.json",
            "vocab.txt",
        ]
    },
}
